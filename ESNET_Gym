import gym
from gym import spaces 
import numpy as np
from ESNET_Env import NetworkGraph
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
import logging
import os
from stable_baselines3 import PPO
from stable_baselines3 import A2C
import matplotlib.pyplot as plt
import time

class EdgeEnv(gym.Env):
    def __init__(self):
        super(EdgeEnv, self).__init__()
        
        self.net = NetworkGraph()
        self.num_change_edges = 15
        self.num_of_edges = self.net.net.number_of_edges()
        self.simTime = 20

        self.action_space = spaces.Discrete(self.num_change_edges) 
        #sample action
        #print("Sample action: ",self.action_space.sample())

        self.observation_space = spaces.Box(low=0.0, high=14.0,shape=(self.num_of_edges,3),dtype=np.float32)

       
        #print sample observation
        #print("Sample observation: ",self.observation_space.sample())

        self.episode_over = False
        self.info = {}
        self.take_action(self.action_space.sample())
 
    def step(self,action):

        if self.simTime <= 0:
            self.episode_over = True
        else:
            self.episode_over = False
 
        prev_bc_links = self.net.get_state(self.net.net)

        network_copy = self.net.net.copy()
        new_net = self.net.randomize_link_weights(self.num_change_edges,network_copy)

        curr_bc_links = self.net.get_state(new_net)

        reward = 0

        #get top 5 src,dst pairs from prev_bc_links based on bc value
        prev_top_links = sorted(prev_bc_links, key=lambda x: x[2], reverse=True)[:5]
        curr_top_links = sorted(curr_bc_links, key=lambda x: x[2], reverse=True)[:5]

        #drop the bc value from the list
        prev_top_links = [x[:2] for x in prev_top_links]
        curr_top_links = [x[:2] for x in curr_top_links]

        #if the src,dst pair is in the top 5 of both lists, then reward -=10
        for link in prev_top_links:
            if link in curr_top_links:
                reward -= 10

        #if all links were changed, this is our optimal solution so reward heavily
        if reward == 0:
            reward += 10000
        #if none of the links were changed, this is our worst solution so penalize heavily
        elif reward == -50:
            reward -= 10000
        else:
            #if some links were changed, reward based on the number of links changed
            reward += 50
       
        self.simTime -= 1

        info = {'delay': self.net.delay(self.net.net)}
        return np.array(curr_bc_links,dtype=np.float32), reward, self.episode_over,info

    def take_action(self,action):
        #prev_obs = self.net.get_state(self.num_change_edges,self.net.net)
        new_net = self.net.randomize_link_weights(action,self.net.net)
        observation = self.net.get_state(new_net)

        #print("Prev_obs",prev_obs)
        #print("Curr obs",observation)
        return observation

    def reset(self):
        self.done = False
        self.net.init_graph()
        self.simTime = 20
        observation = np.array(self.net.get_state(self.net.net),dtype=np.float32)
        
        

        #print(observation)
        return observation

    def render(self):
        #no render currently implemented
        pass

def GenBoxPlot(org_delay,rand_delay,graphname):
    fig = plt.figure()
    fig.suptitle('Packet Delay Distribution')
    plt.boxplot([org_delay,rand_delay])
    plt.xticks([1,2],['org',graphname])
    plt.ylim([0,30])
    plt.grid()
    plt.show()

def main():
    edge_env = EdgeEnv()
    edge_env.step(edge_env.action_space.sample())
    #call env reset
    #edge_env.reset()
    #check_env(edge_env, warn=True)
    
    #get orginal delay
    #org_delay = edge_env.net.delay(edge_env.net.nodeSets,edge_env.net.net)
    episodes = 5
    #delay = []
    """for episode in range(1, episodes+1):
        state = edge_env.reset()
        done = False
        score = 0

        while not done:
            action = edge_env.action_space.sample()
            n_state, reward, done, info = edge_env.step(action)

            score += reward
        print('Episode:{} Score:{}'.format(episode, score))"""
        


   

    log_path = os.path.join('Training', 'Logs')
    #batch size is determined because it needs to be a factor of n_steps * n_envs which is 2048. n_Steps = 2048 and n_Envs = 1
    model = PPO("MlpPolicy", edge_env, batch_size = 128,verbose=1, tensorboard_log=log_path, device = 'cpu')
    model.learn(total_timesteps=500000)
    name = "ppo_1edge_500k_" + time.strftime("%m%d-%H%M")
    model.save(name)
    

if __name__ == '__main__':
    main()
