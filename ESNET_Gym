import gym
from gym import spaces 
import numpy as np
from ESNET_Env import NetworkGraph
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
import logging
import os
from stable_baselines3 import PPO
from stable_baselines3 import A2C
import matplotlib.pyplot as plt
import time
import networkx as nx

class EdgeEnv(gym.Env):
    def __init__(self):
        super(EdgeEnv, self).__init__()
        
        self.net = NetworkGraph()
        self.num_change_edges = 15
        self.num_of_edges = self.net.net.number_of_edges()
        self.simTime = 20

        self.action_space = spaces.Discrete(self.num_change_edges) 
        #sample action
        #print("Sample action: ",self.action_space.sample())

        self.observation_space = spaces.Box(low=0.0, high=14.0,shape=(self.num_of_edges,3),dtype=np.float32)

       
        #print sample observation
        #print("Sample observation: ",self.observation_space.sample())

        self.episode_over = False
        self.info = {}
        self.take_action(self.action_space.sample())
 
    def step(self,action):

        if self.simTime <= 0:
            self.episode_over = True
        else:
            self.episode_over = False
 
        new_net = self.take_action(action)
        curr_state = self.net.get_state(new_net)

        prev_state = self.net.get_state(self.net.net)

        #find the shortest path in prev_state network for all nodes
        prev_shortest_paths = []
        for i in range(0,len(prev_state)):
            prev_shortest_paths.append(nx.dijkstra_path(self.net.net,prev_state[i][0],prev_state[i][1]))
        
        #find the shortest path in curr_state network for all nodes
        curr_shortest_paths = []
        for i in range(0,len(curr_state)):
            curr_shortest_paths.append(nx.dijkstra_path(new_net,curr_state[i][0],curr_state[i][1]))

        print("prev shortest paths: ",prev_shortest_paths)
        print("curr shortest paths: ",curr_shortest_paths)


        #reward for special flow

        prev_flow = self.get_flow_weight(self.net.net)
        curr_flow = self.get_flow_weight(new_net)

        if curr_flow == prev_flow:
            reward += 100000
        else:
            reward -= 100000

        self.simTime -= 1

        info = {'delay': self.net.delay(self.net.net)}
        return np.array(curr_state,dtype=np.float32), reward, self.episode_over,info

    def get_flow_weight(self,network):
        #get weight of path of net.net.flow
        path = self.net.flow
        total_weight = 0
        for i in range(0,len(path)-1):
            total_weight += network[path[i]][path[i+1]]['weight']
        return total_weight
       

    def take_action(self,action):
        #prev_obs = self.net.get_state(self.num_change_edges,self.net.net)
        new_net = self.net.randomize_link_weights(action,self.net.net)
        observation = self.net.get_state(new_net)

        #print("Prev_obs",prev_obs)
        #print("Curr obs",observation)
        return observation

    def reset(self):
        self.done = False
        self.net.init_graph()
        self.simTime = 20
        observation = np.array(self.net.get_state(self.net.net),dtype=np.float32)
        
        

        #print(observation)
        return observation

    def render(self):
        #no render currently implemented
        pass

def GenBoxPlot(org_delay,rand_delay,graphname):
    fig = plt.figure()
    fig.suptitle('Packet Delay Distribution')
    plt.boxplot([org_delay,rand_delay])
    plt.xticks([1,2],['org',graphname])
    plt.ylim([0,30])
    plt.grid()
    plt.show()

def main():
    edge_env = EdgeEnv()
    edge_env.step(edge_env.action_space.sample())
    #call env reset
    #edge_env.reset()
    #check_env(edge_env, warn=True)
    
    #get orginal delay
    #org_delay = edge_env.net.delay(edge_env.net.nodeSets,edge_env.net.net)
    episodes = 5
    #delay = []
    for episode in range(1, episodes+1):
        state = edge_env.reset()
        done = False
        score = 0

        while not done:
            action = edge_env.action_space.sample()
            n_state, reward, done, info = edge_env.step(action)

            score += reward
        print('Episode:{} Score:{}'.format(episode, score))
        


   

    """log_path = os.path.join('Training', 'Logs')
    #batch size is determined because it needs to be a factor of n_steps * n_envs which is 2048. n_Steps = 2048 and n_Envs = 1
    model = PPO("MlpPolicy", edge_env, batch_size = 128,verbose=1, tensorboard_log=log_path, device = 'cpu')
    model.learn(total_timesteps=500000)
    name = "ppo_1edge_flow_500k_" + time.strftime("%m%d-%H%M")
    model.save(name)"""
    

if __name__ == '__main__':
    main()
